import os
import time
import requests
from duckduckgo_search import DDGS
from urllib.parse import urlparse
import mimetypes
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

def create_session():
    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    })
    return session

def get_extension_from_url_or_content(url, response):
    path = urlparse(url).path
    ext = os.path.splitext(path)[1]
    if ext and len(ext) <= 5:
        return ext.lstrip(".")
    content_type = response.headers.get("content-type", "").split(";")[0]
    guessed = mimetypes.guess_extension(content_type)
    if guessed:
        return guessed.lstrip(".")
    return "jpg"

def download_image(url, folder, person_name, seen_hashes, session, bad_domains, lock):
    parsed = urlparse(url)
    if parsed.netloc in bad_domains:
        return None

    try:
        response = session.get(url, stream=True, timeout=(5, 15), verify=False)
        if response.status_code == 200 and response.headers.get("content-type", "").startswith("image"):
            img_bytes = response.content
            img_hash = hashlib.md5(img_bytes).hexdigest()

            with lock:
                if img_hash in seen_hashes:
                    return None
                seen_hashes.add(img_hash)

            ext = get_extension_from_url_or_content(url, response)
            file_path = os.path.join(folder, f"{person_name.replace(' ', '_')}_{img_hash}.{ext}")
            with open(file_path, "wb") as f:
                f.write(img_bytes)
            return file_path
    except Exception:
        bad_domains.add(parsed.netloc)
    return None

def fetch_image_urls(keyword, total_needed, seen_urls):
    with DDGS() as ddgs:
        results = ddgs.images(keywords=keyword, max_results=total_needed*5, safesearch="Off", size="Large")
        urls = []
        for r in results:
            url = r.get("image")
            if url and url not in seen_urls:
                seen_urls.add(url)
                urls.append(url)
            if len(urls) >= total_needed:
                break
        return urls

def download_images(search_terms, save_name, num_images=1000, save_folder="images", threads=10, batch_size=300, pause_time=60):
    folder = os.path.join(save_folder, save_name.replace(" ", "_"))
    os.makedirs(folder, exist_ok=True)

    session = create_session()
    seen_hashes = set()
    seen_urls = set()
    bad_domains = set()
    lock = Lock()
    downloaded_count = 0
    batch_number = 0
    term_index = 0

    print(f"\nüîç Starting download of {num_images} images for: {save_name}")

    while downloaded_count < num_images:
        keyword = search_terms[term_index % len(search_terms)]
        remaining = num_images - downloaded_count
        batch = min(batch_size, remaining)
        print(f"\nüöÄ Batch {batch_number + 1}: Searching for '{keyword}' to fetch {batch} images...")

        image_urls = fetch_image_urls(keyword, batch, seen_urls)
        if not image_urls:
            print(f"‚ùå No more results for '{keyword}'. Trying next keyword...")
            term_index += 1
            if term_index >= len(search_terms):
                print("üõë All search terms exhausted.")
                break
            continue

        with ThreadPoolExecutor(max_workers=threads) as executor:
            future_to_url = {
                executor.submit(download_image, url, folder, save_name, seen_hashes, session, bad_domains, lock): url
                for url in image_urls
            }

            for future in as_completed(future_to_url):
                result = future.result()
                if result:
                    downloaded_count += 1
                    print(f"‚úÖ Downloaded [{downloaded_count}/{num_images}] ‚Üí {os.path.basename(result)}")
                if downloaded_count >= num_images:
                    break

        batch_number += 1
        term_index += 1

        if downloaded_count < num_images:
            print("üïê Taking a 60-second break before next batch...\n")
            time.sleep(pause_time)

    print(f"\nüéâ Done! Downloaded {downloaded_count} images of '{save_name}' to ‚Üí {folder}")

if __name__ == "__main__":
    # üîΩ Example usage
    download_images(
        search_terms=[
            "Benedict Cumberbatch",
            "Benedict Cumberbatch awards",
            "Benedict Cumberbatch actor",
            "Benedict Cumberbatch potraits",
            "Benedict Cumberbatch hd",
            "Benedict Cumberbatch movie scenes",
            "Benedict Cumberbatch photoshoot",
        ],
        save_name="Benedict Cumberbatch",
        num_images=1000
    )
